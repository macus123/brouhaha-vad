{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jun Meng\\Documents\\GitHub\\brouhaha-vad\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torchaudio\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Jzuluaga/atco2_corpus_1h\")[\"test\"].train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "main_dir = \"STT/data\"\n",
    "audio_dir = os.path.join(main_dir, \"audio\")\n",
    "ref_dir = os.path.join(main_dir, \"ref\")\n",
    "audio_test_dir = os.path.join(main_dir, \"audio_test\")\n",
    "ref_test_dir = os.path.join(main_dir, \"ref_test\")\n",
    "sample_rate = 16000\n",
    "test_ratio = 0.2  # Adjust as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ STT-compatible VAD export complete:\n",
      "  TRAIN: 626\n",
      "  DEV:   70\n",
      "  TEST:  175\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# === CONFIG ===\n",
    "vad_audio_base = \"VAD_Input/Audio\"\n",
    "vad_ground_dir = \"VAD_Input/Ground\"\n",
    "sample_rate = 16000\n",
    "test_ratio = 0.2\n",
    "dev_ratio = 0.1  # portion of train set that goes to DEV\n",
    "\n",
    "# Ensure directories exist\n",
    "for d in [os.path.join(vad_audio_base, split) for split in [\"TRAIN\", \"DEV\", \"TEST\"]] + [vad_ground_dir]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# Combine dataset entries\n",
    "if \"all\" in dataset:\n",
    "    all_items = list(dataset[\"all\"])\n",
    "else:\n",
    "    all_items = list(dataset[\"train\"]) + list(dataset[\"test\"])\n",
    "\n",
    "# Filter valid items\n",
    "valid_items = [\n",
    "    item for item in all_items\n",
    "    if all(k in item for k in [\"id\", \"audio\", \"text\", \"segment_start_time\", \"segment_end_time\"])\n",
    "]\n",
    "\n",
    "# Deduplicate by ID\n",
    "unique_items = {str(item[\"id\"]): item for item in valid_items}\n",
    "\n",
    "# === SPLITTING ===\n",
    "all_ids = list(unique_items.keys())\n",
    "train_ids, test_ids = train_test_split(all_ids, test_size=test_ratio, random_state=42)\n",
    "train_ids, dev_ids = train_test_split(train_ids, test_size=dev_ratio, random_state=42)\n",
    "\n",
    "# Ensure disjoint sets\n",
    "train_set = set(train_ids)\n",
    "dev_set = set(dev_ids)\n",
    "test_set = set(test_ids)\n",
    "\n",
    "overlap = (train_set & dev_set) | (train_set & test_set) | (dev_set & test_set)\n",
    "if overlap:\n",
    "    raise ValueError(f\"❌ Overlapping IDs across splits: {overlap}\")\n",
    "\n",
    "# === HELPER FUNCTION ===\n",
    "def save_pair(item, split):\n",
    "    split_dir = os.path.join(vad_audio_base, split.upper())\n",
    "    os.makedirs(split_dir, exist_ok=True)\n",
    "\n",
    "    item_id = str(item[\"id\"])\n",
    "    audio_path = os.path.join(split_dir, f\"{item_id}.wav\")\n",
    "    ref_path = os.path.join(vad_ground_dir, f\"{item_id}.txt\")\n",
    "\n",
    "    # Process audio data\n",
    "    audio_array = item[\"audio\"][\"array\"]\n",
    "    \n",
    "    # Ensure audio data is in the correct range [-1, 1]\n",
    "    if audio_array.max() > 1.0 or audio_array.min() < -1.0:\n",
    "        audio_array = audio_array / max(abs(audio_array.max()), abs(audio_array.min()))\n",
    "    \n",
    "    # Convert to correct format for torchaudio\n",
    "    waveform = torch.tensor(audio_array, dtype=torch.float32).unsqueeze(0)\n",
    "    \n",
    "    # Validate waveform\n",
    "    if torch.isnan(waveform).any():\n",
    "        print(f\"Warning: NaN values found in waveform for {item_id}\")\n",
    "        return\n",
    "    \n",
    "    if waveform.size(1) == 0:\n",
    "        print(f\"Warning: Empty waveform for {item_id}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Save audio with explicit format settings\n",
    "        torchaudio.save(\n",
    "            audio_path, \n",
    "            waveform, \n",
    "            sample_rate=sample_rate,\n",
    "            encoding='PCM_S', \n",
    "            bits_per_sample=16\n",
    "        )\n",
    "        \n",
    "        # Verify the saved file\n",
    "        try:\n",
    "            verification = torchaudio.load(audio_path)\n",
    "            if verification[0].size() != waveform.size():\n",
    "                print(f\"Warning: Size mismatch in saved file for {item_id}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not verify saved file for {item_id}: {e}\")\n",
    "\n",
    "        # Save text with STT-compatible format\n",
    "        with open(ref_path, \"w\") as f:\n",
    "            f.write(f\"{item['segment_start_time']}\\t{item['segment_end_time']}\\t{item['text']}\\n\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {item_id}: {e}\")\n",
    "\n",
    "# === WRITE TO DISK ===\n",
    "for item_id in train_ids:\n",
    "    save_pair(unique_items[item_id], \"TRAIN\")\n",
    "\n",
    "for item_id in dev_ids:\n",
    "    save_pair(unique_items[item_id], \"DEV\")\n",
    "\n",
    "for item_id in test_ids:\n",
    "    save_pair(unique_items[item_id], \"TEST\")\n",
    "\n",
    "print(f\"✅ STT-compatible VAD export complete:\")\n",
    "print(f\"  TRAIN: {len(train_ids)}\")\n",
    "print(f\"  DEV:   {len(dev_ids)}\")\n",
    "print(f\"  TEST:  {len(test_ids)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== File Matching Summary ===\n",
      "\n",
      "DEV:\n",
      "  Total files: 70\n",
      "  Matched: 70\n",
      "  Unmatched: 0\n",
      "\n",
      "TEST:\n",
      "  Total files: 175\n",
      "  Matched: 175\n",
      "  Unmatched: 0\n",
      "\n",
      "TRAIN:\n",
      "  Total files: 626\n",
      "  Matched: 626\n",
      "  Unmatched: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pydub import AudioSegment\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def verify_file_matching():\n",
    "    \"\"\"\n",
    "    Verify that all audio files in DEV, TEST, and TRAIN have matching ground truth files.\n",
    "    Returns dictionaries of matched and unmatched files.\n",
    "    \"\"\"\n",
    "    vad_audio_base = \"VAD_Input/Audio\"\n",
    "    vad_ground_dir = \"VAD_Input/Ground\"\n",
    "    splits = [\"DEV\", \"TEST\", \"TRAIN\"]\n",
    "    \n",
    "    results = {\n",
    "        'matched': {},\n",
    "        'unmatched': {}\n",
    "    }\n",
    "    \n",
    "    # Get all ground truth files\n",
    "    ground_truth_files = set(f.stem for f in Path(vad_ground_dir).glob('*.txt'))\n",
    "    \n",
    "    # Check each split\n",
    "    for split in splits:\n",
    "        audio_dir = Path(vad_audio_base) / split\n",
    "        \n",
    "        # Initialize results for this split\n",
    "        results['matched'][split] = []\n",
    "        results['unmatched'][split] = []\n",
    "        \n",
    "        # Check each audio file\n",
    "        for audio_file in audio_dir.glob('*.wav'):\n",
    "            if audio_file.stem in ground_truth_files:\n",
    "                results['matched'][split].append(audio_file.name)\n",
    "            else:\n",
    "                results['unmatched'][split].append(audio_file.name)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"=== File Matching Summary ===\")\n",
    "    for split in splits:\n",
    "        matched = len(results['matched'][split])\n",
    "        unmatched = len(results['unmatched'][split])\n",
    "        total = matched + unmatched\n",
    "        \n",
    "        print(f\"\\n{split}:\")\n",
    "        print(f\"  Total files: {total}\")\n",
    "        print(f\"  Matched: {matched}\")\n",
    "        print(f\"  Unmatched: {unmatched}\")\n",
    "        \n",
    "        if unmatched > 0:\n",
    "            print(\"\\n  Unmatched files:\")\n",
    "            for file in results['unmatched'][split]:\n",
    "                print(f\"    - {file}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the verification\n",
    "matching_results = verify_file_matching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing 5 Random Files ===\n",
      "\n",
      "\n",
      "File: atco2_test-set-1h_LSZH_ZURICH_ApronS_121_75MHz_20210414_114326-A__000007-000684.wav\n",
      "--------------------------------------------------------------------------------\n",
      "Total duration: 0:00:06.770000\n",
      "\n",
      "Timeline:\n",
      "  0:00:00 → 0:00:00.070000 (0.07s): NON-SPEECH\n",
      "  0:00:00.070000 → 0:00:06.840000 (6.77s): SPEECH\n",
      "    Text: vista jet four three seven turn left via taxiway alfa echo six hold short runway one six\n",
      "\n",
      "To verify this file:\n",
      "1. Open audio file: VAD_Input\\Audio\\TRAIN\\atco2_test-set-1h_LSZH_ZURICH_ApronS_121_75MHz_20210414_114326-A__000007-000684.wav\n",
      "2. Open ground truth: VAD_Input\\Ground\\atco2_test-set-1h_LSZH_ZURICH_ApronS_121_75MHz_20210414_114326-A__000007-000684.txt\n",
      "3. Listen to the audio and check if segments match the timeline above\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "File: atco2_test-set-1h_LSGS_SION_Ground_Control_121_7MHz_20210502_134810-A__000028-000180.wav\n",
      "--------------------------------------------------------------------------------\n",
      "Total duration: 0:00:01.520000\n",
      "\n",
      "Timeline:\n",
      "  0:00:00 → 0:00:00.280000 (0.28s): NON-SPEECH\n",
      "  0:00:00.280000 → 0:00:01.800000 (1.52s): SPEECH\n",
      "    Text: delta whiskey\n",
      "\n",
      "To verify this file:\n",
      "1. Open audio file: VAD_Input\\Audio\\TEST\\atco2_test-set-1h_LSGS_SION_Ground_Control_121_7MHz_20210502_134810-A__000028-000180.wav\n",
      "2. Open ground truth: VAD_Input\\Ground\\atco2_test-set-1h_LSGS_SION_Ground_Control_121_7MHz_20210502_134810-A__000028-000180.txt\n",
      "3. Listen to the audio and check if segments match the timeline above\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "File: atco2_test-set-1h_LSGS_SION_Ground_Control_121_7MHz_20210503_151837-D__000014-000287.wav\n",
      "--------------------------------------------------------------------------------\n",
      "Total duration: 0:00:02.730000\n",
      "\n",
      "Timeline:\n",
      "  0:00:00 → 0:00:00.140000 (0.14s): NON-SPEECH\n",
      "  0:00:00.140000 → 0:00:02.870000 (2.73s): SPEECH\n",
      "    Text: heli charlie zulu we have the traffic in sight\n",
      "\n",
      "To verify this file:\n",
      "1. Open audio file: VAD_Input\\Audio\\DEV\\atco2_test-set-1h_LSGS_SION_Ground_Control_121_7MHz_20210503_151837-D__000014-000287.wav\n",
      "2. Open ground truth: VAD_Input\\Ground\\atco2_test-set-1h_LSGS_SION_Ground_Control_121_7MHz_20210503_151837-D__000014-000287.txt\n",
      "3. Listen to the audio and check if segments match the timeline above\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "File: atco2_test-set-1h_LZIB_STEFANIK_Tower_118_3MHz_20210430_000651-B__000000-000480.wav\n",
      "--------------------------------------------------------------------------------\n",
      "Total duration: 0:00:04.800000\n",
      "\n",
      "Timeline:\n",
      "  0:00:00 → 0:00:04.800000 (4.80s): SPEECH\n",
      "    Text: euro trans five three three five approaching fox one six ready for departure\n",
      "\n",
      "To verify this file:\n",
      "1. Open audio file: VAD_Input\\Audio\\TRAIN\\atco2_test-set-1h_LZIB_STEFANIK_Tower_118_3MHz_20210430_000651-B__000000-000480.wav\n",
      "2. Open ground truth: VAD_Input\\Ground\\atco2_test-set-1h_LZIB_STEFANIK_Tower_118_3MHz_20210430_000651-B__000000-000480.txt\n",
      "3. Listen to the audio and check if segments match the timeline above\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "File: atco2_test-set-1h_LKPR_RUZYNE_Radar_120_520MHz_20201029_051226-B__000698-001140.wav\n",
      "--------------------------------------------------------------------------------\n",
      "Total duration: 0:00:04.420000\n",
      "\n",
      "Timeline:\n",
      "  0:00:00 → 0:00:06.980000 (6.98s): NON-SPEECH\n",
      "  0:00:06.980000 → 0:00:11.400000 (4.42s): SPEECH\n",
      "    Text: one two zero two seven five ahoj klm one three five zero\n",
      "\n",
      "To verify this file:\n",
      "1. Open audio file: VAD_Input\\Audio\\TRAIN\\atco2_test-set-1h_LKPR_RUZYNE_Radar_120_520MHz_20201029_051226-B__000698-001140.wav\n",
      "2. Open ground truth: VAD_Input\\Ground\\atco2_test-set-1h_LKPR_RUZYNE_Radar_120_520MHz_20201029_051226-B__000698-001140.txt\n",
      "3. Listen to the audio and check if segments match the timeline above\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "from pydub import AudioSegment\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def test_segmentation_random_files(n_files=5):\n",
    "    \"\"\"\n",
    "    Test segmentation on n random files and verify that the split WAV files add up to the original file length.\n",
    "    Also compares original and processed ground truth files.\n",
    "    \"\"\"\n",
    "    # Setup paths\n",
    "    input_base_dir = Path(\"VAD_Input\")\n",
    "    output_base_dir = Path(\"VAD_Output\")\n",
    "    splits = [\"DEV\", \"TEST\", \"TRAIN\"]\n",
    "    \n",
    "    # Collect all audio files that have been processed\n",
    "    all_audio_files = []\n",
    "    processed_files = set()\n",
    "    \n",
    "    # Find all processed files first\n",
    "    for split in splits:\n",
    "        speech_dir = output_base_dir / split / \"speech\"\n",
    "        if speech_dir.exists():\n",
    "            for file in speech_dir.glob(\"*.wav\"):\n",
    "                processed_files.add(file.stem)\n",
    "    \n",
    "    # Now find matching original files\n",
    "    for split in splits:\n",
    "        audio_dir = input_base_dir / \"Audio\" / split\n",
    "        for file in audio_dir.glob(\"*.wav\"):\n",
    "            if file.stem in processed_files:\n",
    "                all_audio_files.append((split, file))\n",
    "    \n",
    "    # Randomly select n files\n",
    "    test_files = random.sample(all_audio_files, min(n_files, len(all_audio_files)))\n",
    "    \n",
    "    print(f\"=== Testing {len(test_files)} Random Files ===\\n\")\n",
    "    print(\"🔍 Verification checks:\")\n",
    "    print(\"  ✅ Duration check: Original duration = Speech duration + Non-speech duration\")\n",
    "    print(\"  ✅ Content check: Original audio segments properly sorted into speech/non-speech\")\n",
    "    print(\"  ✅ Ground truth verification: Original and processed ground truth files consistent\\n\")\n",
    "    \n",
    "    result_table = []\n",
    "    \n",
    "    for split, audio_file in test_files:\n",
    "        input_gt_file = input_base_dir / \"Ground\" / f\"{audio_file.stem}.txt\"\n",
    "        speech_output_file = output_base_dir / split / \"speech\" / f\"{audio_file.stem}.wav\"\n",
    "        non_speech_output_file = output_base_dir / split / \"non_speech\" / f\"{audio_file.stem}.wav\"\n",
    "        speech_gt_file = output_base_dir / \"Ground\" / f\"{audio_file.stem}_speech.txt\"\n",
    "        non_speech_gt_file = output_base_dir / \"Ground\" / f\"{audio_file.stem}_non_speech.txt\"\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"📊 File: {audio_file.name} [{split}]\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # === LOAD ALL AUDIO FILES ===\n",
    "        original_audio = AudioSegment.from_file(str(audio_file))\n",
    "        original_duration_ms = len(original_audio)\n",
    "        original_duration_sec = original_duration_ms / 1000.0\n",
    "        \n",
    "        speech_audio = AudioSegment.from_file(str(speech_output_file))\n",
    "        speech_duration_ms = len(speech_audio)\n",
    "        speech_duration_sec = speech_duration_ms / 1000.0\n",
    "        \n",
    "        non_speech_audio = AudioSegment.from_file(str(non_speech_output_file))\n",
    "        non_speech_duration_ms = len(non_speech_audio)\n",
    "        non_speech_duration_sec = non_speech_duration_ms / 1000.0\n",
    "        \n",
    "        combined_duration_ms = speech_duration_ms + non_speech_duration_ms\n",
    "        difference_ms = abs(original_duration_ms - combined_duration_ms)\n",
    "        \n",
    "        # === DURATION VERIFICATION ===\n",
    "        print(\"\\n🕒 DURATION CHECK:\")\n",
    "        print(f\"  Original audio: {timedelta(seconds=original_duration_sec)} ({original_duration_ms} ms)\")\n",
    "        print(f\"  Speech audio:   {timedelta(seconds=speech_duration_sec)} ({speech_duration_ms} ms)\")\n",
    "        print(f\"  Non-speech:     {timedelta(seconds=non_speech_duration_sec)} ({non_speech_duration_ms} ms)\")\n",
    "        print(f\"  Speech + Non-speech: {timedelta(seconds=(combined_duration_ms/1000))} ({combined_duration_ms} ms)\")\n",
    "        \n",
    "        if difference_ms <= 10:  # Allow small rounding differences\n",
    "            print(f\"  ✅ Durations match! Difference: {difference_ms} ms (within tolerance)\")\n",
    "            duration_ok = True\n",
    "        else:\n",
    "            print(f\"  ❌ Durations DO NOT match! Difference: {difference_ms} ms\")\n",
    "            duration_ok = False\n",
    "        \n",
    "        # === GROUND TRUTH VERIFICATION ===\n",
    "        print(\"\\n📄 GROUND TRUTH COMPARISON:\")\n",
    "        \n",
    "        # Read original ground truth\n",
    "        orig_segments = []\n",
    "        with open(input_gt_file, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) >= 2:\n",
    "                    start, end = float(parts[0]), float(parts[1])\n",
    "                    text = parts[2] if len(parts) > 2 else \"\"\n",
    "                    orig_segments.append({'start': start, 'end': end, 'text': text})\n",
    "        \n",
    "        # Read speech ground truth\n",
    "        speech_segments = []\n",
    "        with open(speech_gt_file, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) >= 2:\n",
    "                    start, end = float(parts[0]), float(parts[1])\n",
    "                    text = parts[2] if len(parts) > 2 else \"\"\n",
    "                    speech_segments.append({'start': start, 'end': end, 'text': text})\n",
    "        \n",
    "        # Read non-speech ground truth\n",
    "        non_speech_segments = []\n",
    "        with open(non_speech_gt_file, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) >= 2:\n",
    "                    start, end = float(parts[0]), float(parts[1])\n",
    "                    text = parts[2] if len(parts) > 2 else \"\"\n",
    "                    non_speech_segments.append({'start': start, 'end': end, 'text': text})\n",
    "        \n",
    "        # Calculate total durations from ground truth\n",
    "        orig_duration_from_gt = sum(seg['end'] - seg['start'] for seg in orig_segments)\n",
    "        speech_duration_from_gt = sum(seg['end'] - seg['start'] for seg in speech_segments)\n",
    "        non_speech_duration_from_gt = sum(seg['end'] - seg['start'] for seg in non_speech_segments)\n",
    "        \n",
    "        print(f\"  Original ground truth segments: {len(orig_segments)}\")\n",
    "        print(f\"  Speech ground truth segments: {len(speech_segments)}\")\n",
    "        print(f\"  Non-speech ground truth segments: {len(non_speech_segments)}\")\n",
    "        print(f\"  Original duration from GT: {orig_duration_from_gt:.2f}s\")\n",
    "        print(f\"  Speech duration from GT: {speech_duration_from_gt:.2f}s\")\n",
    "        print(f\"  Non-speech duration from GT: {non_speech_duration_from_gt:.2f}s\")\n",
    "        \n",
    "        # Verify if speech segments in processed ground truth match original\n",
    "        speech_gt_match = True\n",
    "        for seg in speech_segments:\n",
    "            match_found = False\n",
    "            for orig_seg in orig_segments:\n",
    "                # Use a small epsilon for float comparison\n",
    "                if abs(seg['start'] - orig_seg['start']) < 0.001 and abs(seg['end'] - orig_seg['end']) < 0.001:\n",
    "                    match_found = True\n",
    "                    break\n",
    "            if not match_found:\n",
    "                speech_gt_match = False\n",
    "                print(f\"  ⚠️ Speech segment not found in original GT: {seg['start']} - {seg['end']}\")\n",
    "        \n",
    "        if speech_gt_match:\n",
    "            print(\"  ✅ Speech ground truth segments match original!\")\n",
    "        \n",
    "        # Verify that combined speech and non-speech segments cover the entire audio\n",
    "        combined_duration_from_gt = speech_duration_from_gt + non_speech_duration_from_gt\n",
    "        gt_diff = abs(original_duration_sec - combined_duration_from_gt)\n",
    "        \n",
    "        if gt_diff < 0.1:  # Allow small rounding differences\n",
    "            print(f\"  ✅ Ground truth durations add up! Difference: {gt_diff:.3f}s\")\n",
    "            gt_ok = True\n",
    "        else:\n",
    "            print(f\"  ❌ Ground truth durations DO NOT add up! Difference: {gt_diff:.3f}s\")\n",
    "            gt_ok = False\n",
    "        \n",
    "        # === TIMELINE VISUALIZATION ===\n",
    "        print(\"\\n⏱️ TIMELINE VIEW:\")\n",
    "        print(\"  Original segments:\")\n",
    "        timeline = []\n",
    "        current_time = 0\n",
    "        for i, seg in enumerate(sorted(orig_segments, key=lambda x: x['start'])):\n",
    "            if current_time < seg['start']:\n",
    "                timeline.append({\n",
    "                    'start': current_time,\n",
    "                    'end': seg['start'],\n",
    "                    'type': 'NON-SPEECH',\n",
    "                    'text': ''\n",
    "                })\n",
    "            timeline.append({\n",
    "                'start': seg['start'],\n",
    "                'end': seg['end'],\n",
    "                'type': 'SPEECH',\n",
    "                'text': seg['text']\n",
    "            })\n",
    "            current_time = seg['end']\n",
    "        \n",
    "        if current_time < original_duration_sec:\n",
    "            timeline.append({\n",
    "                'start': current_time,\n",
    "                'end': original_duration_sec,\n",
    "                'type': 'NON-SPEECH',\n",
    "                'text': ''\n",
    "            })\n",
    "        \n",
    "        for event in timeline:\n",
    "            duration = event['end'] - event['start']\n",
    "            start_time = str(timedelta(seconds=event['start'])).split('.')[0]\n",
    "            end_time = str(timedelta(seconds=event['end'])).split('.')[0]\n",
    "            \n",
    "            if event['type'] == 'SPEECH':\n",
    "                print(f\"  {start_time} → {end_time} ({duration:.2f}s): 🗣️ {event['type']}\")\n",
    "                if event['text']:\n",
    "                    print(f\"    📝 {event['text'][:60]}{'...' if len(event['text']) > 60 else ''}\")\n",
    "            else:\n",
    "                print(f\"  {start_time} → {end_time} ({duration:.2f}s): 🔇 {event['type']}\")\n",
    "        \n",
    "        # === FILE PATHS FOR REFERENCE ===\n",
    "        print(\"\\n📁 FILE REFERENCES:\")\n",
    "        print(f\"  Original audio:        {audio_file}\")\n",
    "        print(f\"  Original ground truth: {input_gt_file}\")\n",
    "        print(f\"  Speech audio:          {speech_output_file}\")\n",
    "        print(f\"  Non-speech audio:      {non_speech_output_file}\")\n",
    "        print(f\"  Speech ground truth:   {speech_gt_file}\")\n",
    "        print(f\"  Non-speech ground truth: {non_speech_gt_file}\")\n",
    "        \n",
    "        # Add to results table\n",
    "        result_table.append({\n",
    "            'File': audio_file.name,\n",
    "            'Split': split,\n",
    "            'Original (s)': original_duration_sec,\n",
    "            'Speech (s)': speech_duration_sec,\n",
    "            'Non-speech (s)': non_speech_duration_sec,\n",
    "            'Sum (s)': speech_duration_sec + non_speech_duration_sec,\n",
    "            'Difference (ms)': difference_ms,\n",
    "            'Duration OK': '✅' if duration_ok else '❌',\n",
    "            'GT OK': '✅' if gt_ok else '❌'\n",
    "        })\n",
    "    \n",
    "    # === SUMMARY TABLE ===\n",
    "    if result_table:\n",
    "        print(\"\\n\\n📊 SUMMARY OF TESTED FILES:\")\n",
    "        df = pd.DataFrame(result_table)\n",
    "        print(df.to_string(index=False))\n",
    "        \n",
    "        # Overall verdict\n",
    "        all_duration_ok = all(result['Duration OK'] == '✅' for result in result_table)\n",
    "        all_gt_ok = all(result['GT OK'] == '✅' for result in result_table)\n",
    "        \n",
    "        print(\"\\n🏁 FINAL VERDICT:\")\n",
    "        if all_duration_ok and all_gt_ok:\n",
    "            print(\"  ✅ ALL TESTS PASSED! Audio splitting appears to be working correctly.\")\n",
    "        else:\n",
    "            print(\"  ❌ SOME TESTS FAILED. Check the details above.\")\n",
    "            if not all_duration_ok:\n",
    "                print(\"    - Duration matching issues detected\")\n",
    "            if not all_gt_ok:\n",
    "                print(\"    - Ground truth consistency issues detected\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    random.seed(42)  # For reproducibility\n",
    "    test_segmentation_random_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Single File ===\n",
      "Audio file: atco2_test-set-1h_LKPR_RUZYNE_Radar_120_520MHz_20201026_145941-G__001153-001308.wav\n",
      "Ground truth: atco2_test-set-1h_LKPR_RUZYNE_Radar_120_520MHz_20201026_145941-G__001153-001308.txt\n",
      "Error processing file: [Errno 2] No such file or directory: 'VAD_Input\\\\Audio\\\\DEV\\\\atco2_test-set-1h_LKPR_RUZYNE_Radar_120_520MHz_20201026_145941-G__001153-001308.wav'\n",
      "Processing failed!\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from pydub import AudioSegment\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "def process_audio_file(audio_file, gt_file, speech_dir, non_speech_dir):\n",
    "    \"\"\"\n",
    "    Process a single audio file and split it into speech/non-speech segments.\n",
    "    Returns the processed segments and their metadata.\n",
    "    \"\"\"\n",
    "    # Load audio file\n",
    "    audio = AudioSegment.from_file(str(audio_file))\n",
    "    total_duration = len(audio) / 1000.0\n",
    "    \n",
    "    # Read and sort ground truth segments\n",
    "    speech_segments = []\n",
    "    with open(gt_file, 'r') as f:\n",
    "        for line in f:\n",
    "            start, end, text = line.strip().split('\\t')\n",
    "            speech_segments.append({\n",
    "                'start': float(start),\n",
    "                'end': float(end),\n",
    "                'text': text\n",
    "            })\n",
    "    speech_segments.sort(key=lambda x: x['start'])\n",
    "    \n",
    "    # Process speech segments\n",
    "    combined_speech = AudioSegment.empty()\n",
    "    speech_metadata = []\n",
    "    for seg in speech_segments:\n",
    "        start_ms = int(seg['start'] * 1000)\n",
    "        end_ms = int(seg['end'] * 1000)\n",
    "        segment = audio[start_ms:end_ms]\n",
    "        combined_speech += segment\n",
    "        speech_metadata.append(seg)\n",
    "    \n",
    "    # Save speech segments\n",
    "    speech_output = speech_dir / f\"{audio_file.stem}_speech.wav\"\n",
    "    combined_speech.export(str(speech_output), format=\"wav\", parameters=[\"-ac\", \"1\"])\n",
    "    \n",
    "    # Process non-speech segments\n",
    "    current_time = 0.0\n",
    "    combined_non_speech = AudioSegment.empty()\n",
    "    non_speech_metadata = []\n",
    "    \n",
    "    for seg in speech_segments:\n",
    "        if current_time < seg['start']:\n",
    "            start_ms = int(current_time * 1000)\n",
    "            end_ms = int(seg['start'] * 1000)\n",
    "            non_speech_segment = audio[start_ms:end_ms]\n",
    "            combined_non_speech += non_speech_segment\n",
    "            non_speech_metadata.append({\n",
    "                'start': current_time,\n",
    "                'end': seg['start']\n",
    "            })\n",
    "        current_time = seg['end']\n",
    "    \n",
    "    # Handle final non-speech segment\n",
    "    if current_time < total_duration:\n",
    "        start_ms = int(current_time * 1000)\n",
    "        end_ms = int(total_duration * 1000)\n",
    "        non_speech_segment = audio[start_ms:end_ms]\n",
    "        combined_non_speech += non_speech_segment\n",
    "        non_speech_metadata.append({\n",
    "            'start': current_time,\n",
    "            'end': total_duration\n",
    "        })\n",
    "    \n",
    "    # Save non-speech segments\n",
    "    non_speech_output = non_speech_dir / f\"{audio_file.stem}_non_speech.wav\"\n",
    "    combined_non_speech.export(str(non_speech_output), format=\"wav\", parameters=[\"-ac\", \"1\"])\n",
    "    \n",
    "    return {\n",
    "        'total_duration': total_duration,\n",
    "        'speech_segments': speech_metadata,\n",
    "        'non_speech_segments': non_speech_metadata,\n",
    "        'speech_file': speech_output,\n",
    "        'non_speech_file': non_speech_output\n",
    "    }\n",
    "\n",
    "def print_metadata(audio_file, metadata):\n",
    "    \"\"\"Print formatted metadata for the processed file.\"\"\"\n",
    "    print(\"\\n=== Metadata ===\")\n",
    "    print(f\"Original file: {audio_file.name}\")\n",
    "    print(f\"Original duration: {timedelta(seconds=metadata['total_duration'])}\")\n",
    "    \n",
    "    print(\"\\nSpeech segments:\")\n",
    "    for seg in metadata['speech_segments']:\n",
    "        print(f\"[{timedelta(seconds=seg['start'])} → {timedelta(seconds=seg['end'])}] \"\n",
    "              f\"Text: {seg['text']}\")\n",
    "    \n",
    "    print(\"\\nNon-speech segments:\")\n",
    "    for seg in metadata['non_speech_segments']:\n",
    "        print(f\"[{timedelta(seconds=seg['start'])} → {timedelta(seconds=seg['end'])}]\")\n",
    "    \n",
    "    print(f\"\\nOutput files:\")\n",
    "    print(f\"Speech file: {metadata['speech_file'].name}\")\n",
    "    print(f\"Non-speech file: {metadata['non_speech_file'].name}\")\n",
    "\n",
    "def test_single_file_split(audio_path, gt_path, output_dir=\"test_output\"):\n",
    "    \"\"\"Test splitting functionality on a single audio/ground truth pair.\"\"\"\n",
    "    try:\n",
    "        # Setup paths\n",
    "        audio_file = Path(audio_path)\n",
    "        gt_file = Path(gt_path)\n",
    "        output_dir = Path(output_dir)\n",
    "        speech_dir = output_dir / \"speech\"\n",
    "        non_speech_dir = output_dir / \"non_speech\"\n",
    "        \n",
    "        # Create output directories\n",
    "        speech_dir.mkdir(parents=True, exist_ok=True)\n",
    "        non_speech_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(\"\\n=== Processing Single File ===\")\n",
    "        print(f\"Audio file: {audio_file.name}\")\n",
    "        print(f\"Ground truth: {gt_file.name}\")\n",
    "        \n",
    "        # Process the audio file\n",
    "        metadata = process_audio_file(audio_file, gt_file, speech_dir, non_speech_dir)\n",
    "        \n",
    "        # Print metadata after processing is complete\n",
    "        print_metadata(audio_file, metadata)\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    audio_file = \"VAD_Input/Audio/DEV/atco2_test-set-1h_LKPR_RUZYNE_Radar_120_520MHz_20201026_145941-G__001153-001308.wav\"\n",
    "    gt_file = \"VAD_Input/Ground/atco2_test-set-1h_LKPR_RUZYNE_Radar_120_520MHz_20201026_145941-G__001153-001308.txt\"\n",
    "    \n",
    "    success = test_single_file_split(audio_file, gt_file)\n",
    "    if not success:\n",
    "        print(\"Processing failed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
