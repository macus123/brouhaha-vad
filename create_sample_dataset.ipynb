{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jun Meng\\Documents\\GitHub\\brouhaha-vad\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torchaudio\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Jzuluaga/atco2_corpus_1h\")[\"test\"].train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "main_dir = \"STT/data\"\n",
    "audio_dir = os.path.join(main_dir, \"audio\")\n",
    "ref_dir = os.path.join(main_dir, \"ref\")\n",
    "audio_test_dir = os.path.join(main_dir, \"audio_test\")\n",
    "ref_test_dir = os.path.join(main_dir, \"ref_test\")\n",
    "sample_rate = 16000\n",
    "test_ratio = 0.2  # Adjust as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ STT-compatible VAD export complete:\n",
      "  TRAIN: 626\n",
      "  DEV:   70\n",
      "  TEST:  175\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# === CONFIG ===\n",
    "vad_audio_base = \"VAD_Input/Audio\"\n",
    "vad_ground_dir = \"VAD_Input/Ground\"\n",
    "sample_rate = 16000\n",
    "test_ratio = 0.2\n",
    "dev_ratio = 0.1  # portion of train set that goes to DEV\n",
    "\n",
    "# Ensure directories exist\n",
    "for d in [os.path.join(vad_audio_base, split) for split in [\"TRAIN\", \"DEV\", \"TEST\"]] + [vad_ground_dir]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# Combine dataset entries\n",
    "if \"all\" in dataset:\n",
    "    all_items = list(dataset[\"all\"])\n",
    "else:\n",
    "    all_items = list(dataset[\"train\"]) + list(dataset[\"test\"])\n",
    "\n",
    "# Filter valid items\n",
    "valid_items = [\n",
    "    item for item in all_items\n",
    "    if all(k in item for k in [\"id\", \"audio\", \"text\", \"segment_start_time\", \"segment_end_time\"])\n",
    "]\n",
    "\n",
    "# Deduplicate by ID\n",
    "unique_items = {str(item[\"id\"]): item for item in valid_items}\n",
    "\n",
    "# === SPLITTING ===\n",
    "all_ids = list(unique_items.keys())\n",
    "train_ids, test_ids = train_test_split(all_ids, test_size=test_ratio, random_state=42)\n",
    "train_ids, dev_ids = train_test_split(train_ids, test_size=dev_ratio, random_state=42)\n",
    "\n",
    "# Ensure disjoint sets\n",
    "train_set = set(train_ids)\n",
    "dev_set = set(dev_ids)\n",
    "test_set = set(test_ids)\n",
    "\n",
    "overlap = (train_set & dev_set) | (train_set & test_set) | (dev_set & test_set)\n",
    "if overlap:\n",
    "    raise ValueError(f\"❌ Overlapping IDs across splits: {overlap}\")\n",
    "\n",
    "# === HELPER FUNCTION ===\n",
    "def save_pair(item, split):\n",
    "    split_dir = os.path.join(vad_audio_base, split.upper())\n",
    "    os.makedirs(split_dir, exist_ok=True)\n",
    "\n",
    "    item_id = str(item[\"id\"])\n",
    "    audio_path = os.path.join(split_dir, f\"{item_id}.wav\")\n",
    "    ref_path = os.path.join(vad_ground_dir, f\"{item_id}.txt\")\n",
    "\n",
    "    # Process audio data\n",
    "    audio_array = item[\"audio\"][\"array\"]\n",
    "    \n",
    "    # Ensure audio data is in the correct range [-1, 1]\n",
    "    if audio_array.max() > 1.0 or audio_array.min() < -1.0:\n",
    "        audio_array = audio_array / max(abs(audio_array.max()), abs(audio_array.min()))\n",
    "    \n",
    "    # Convert to correct format for torchaudio\n",
    "    waveform = torch.tensor(audio_array, dtype=torch.float32).unsqueeze(0)\n",
    "    \n",
    "    # Validate waveform\n",
    "    if torch.isnan(waveform).any():\n",
    "        print(f\"Warning: NaN values found in waveform for {item_id}\")\n",
    "        return\n",
    "    \n",
    "    if waveform.size(1) == 0:\n",
    "        print(f\"Warning: Empty waveform for {item_id}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Save audio with explicit format settings\n",
    "        torchaudio.save(\n",
    "            audio_path, \n",
    "            waveform, \n",
    "            sample_rate=sample_rate,\n",
    "            encoding='PCM_S', \n",
    "            bits_per_sample=16\n",
    "        )\n",
    "        \n",
    "        # Verify the saved file\n",
    "        try:\n",
    "            verification = torchaudio.load(audio_path)\n",
    "            if verification[0].size() != waveform.size():\n",
    "                print(f\"Warning: Size mismatch in saved file for {item_id}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not verify saved file for {item_id}: {e}\")\n",
    "\n",
    "        # Save text with STT-compatible format\n",
    "        with open(ref_path, \"w\") as f:\n",
    "            f.write(f\"{item['segment_start_time']}\\t{item['segment_end_time']}\\t{item['text']}\\n\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {item_id}: {e}\")\n",
    "\n",
    "# === WRITE TO DISK ===\n",
    "for item_id in train_ids:\n",
    "    save_pair(unique_items[item_id], \"TRAIN\")\n",
    "\n",
    "for item_id in dev_ids:\n",
    "    save_pair(unique_items[item_id], \"DEV\")\n",
    "\n",
    "for item_id in test_ids:\n",
    "    save_pair(unique_items[item_id], \"TEST\")\n",
    "\n",
    "print(f\"✅ STT-compatible VAD export complete:\")\n",
    "print(f\"  TRAIN: {len(train_ids)}\")\n",
    "print(f\"  DEV:   {len(dev_ids)}\")\n",
    "print(f\"  TEST:  {len(test_ids)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== File Matching Summary ===\n",
      "\n",
      "DEV:\n",
      "  Total files: 70\n",
      "  Matched: 70\n",
      "  Unmatched: 0\n",
      "\n",
      "TEST:\n",
      "  Total files: 175\n",
      "  Matched: 175\n",
      "  Unmatched: 0\n",
      "\n",
      "TRAIN:\n",
      "  Total files: 626\n",
      "  Matched: 626\n",
      "  Unmatched: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pydub import AudioSegment\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def verify_file_matching():\n",
    "    \"\"\"\n",
    "    Verify that all audio files in DEV, TEST, and TRAIN have matching ground truth files.\n",
    "    Returns dictionaries of matched and unmatched files.\n",
    "    \"\"\"\n",
    "    vad_audio_base = \"VAD_Input/Audio\"\n",
    "    vad_ground_dir = \"VAD_Input/Ground\"\n",
    "    splits = [\"DEV\", \"TEST\", \"TRAIN\"]\n",
    "    \n",
    "    results = {\n",
    "        'matched': {},\n",
    "        'unmatched': {}\n",
    "    }\n",
    "    \n",
    "    # Get all ground truth files\n",
    "    ground_truth_files = set(f.stem for f in Path(vad_ground_dir).glob('*.txt'))\n",
    "    \n",
    "    # Check each split\n",
    "    for split in splits:\n",
    "        audio_dir = Path(vad_audio_base) / split\n",
    "        \n",
    "        # Initialize results for this split\n",
    "        results['matched'][split] = []\n",
    "        results['unmatched'][split] = []\n",
    "        \n",
    "        # Check each audio file\n",
    "        for audio_file in audio_dir.glob('*.wav'):\n",
    "            if audio_file.stem in ground_truth_files:\n",
    "                results['matched'][split].append(audio_file.name)\n",
    "            else:\n",
    "                results['unmatched'][split].append(audio_file.name)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"=== File Matching Summary ===\")\n",
    "    for split in splits:\n",
    "        matched = len(results['matched'][split])\n",
    "        unmatched = len(results['unmatched'][split])\n",
    "        total = matched + unmatched\n",
    "        \n",
    "        print(f\"\\n{split}:\")\n",
    "        print(f\"  Total files: {total}\")\n",
    "        print(f\"  Matched: {matched}\")\n",
    "        print(f\"  Unmatched: {unmatched}\")\n",
    "        \n",
    "        if unmatched > 0:\n",
    "            print(\"\\n  Unmatched files:\")\n",
    "            for file in results['unmatched'][split]:\n",
    "                print(f\"    - {file}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the verification\n",
    "matching_results = verify_file_matching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing 5 Random Files ===\n",
      "\n",
      "\n",
      "File: atco2_test-set-1h_LSZH_ZURICH_ApronN_121_85MHz_20210414_081643-A__000007-000263.wav\n",
      "--------------------------------------------------------------------------------\n",
      "Total duration: 0:00:02.560000\n",
      "\n",
      "Timeline:\n",
      "  0:00:00 → 0:00:00.070000 (0.07s): NON-SPEECH\n",
      "  0:00:00.070000 → 0:00:02.630000 (2.56s): SPEECH\n",
      "    Text: turkish two seven xray tower one one eight one bye bye\n",
      "\n",
      "To verify this file:\n",
      "1. Open audio file: VAD_Input\\Audio\\TRAIN\\atco2_test-set-1h_LSZH_ZURICH_ApronN_121_85MHz_20210414_081643-A__000007-000263.wav\n",
      "2. Open ground truth: VAD_Input\\Ground\\atco2_test-set-1h_LSZH_ZURICH_ApronN_121_85MHz_20210414_081643-A__000007-000263.txt\n",
      "3. Listen to the audio and check if segments match the timeline above\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "File: atco2_test-set-1h_LSGS_SION_Ground_Control_121_7MHz_20210502_164218-A__000189-000834.wav\n",
      "--------------------------------------------------------------------------------\n",
      "Total duration: 0:00:06.450000\n",
      "\n",
      "Timeline:\n",
      "  0:00:00 → 0:00:01.890000 (1.89s): NON-SPEECH\n",
      "  0:00:01.890000 → 0:00:08.340000 (6.45s): SPEECH\n",
      "    Text: lufthansa mike continue for what two zero meters and then will be first a yellow light to the right\n",
      "\n",
      "To verify this file:\n",
      "1. Open audio file: VAD_Input\\Audio\\TEST\\atco2_test-set-1h_LSGS_SION_Ground_Control_121_7MHz_20210502_164218-A__000189-000834.wav\n",
      "2. Open ground truth: VAD_Input\\Ground\\atco2_test-set-1h_LSGS_SION_Ground_Control_121_7MHz_20210502_164218-A__000189-000834.txt\n",
      "3. Listen to the audio and check if segments match the timeline above\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "File: atco2_test-set-1h_LSGS_SION_Tower_118_3MHz_20210423_093650-D__000016-000381.wav\n",
      "--------------------------------------------------------------------------------\n",
      "Total duration: 0:00:03.650000\n",
      "\n",
      "Timeline:\n",
      "  0:00:00 → 0:00:00.160000 (0.16s): NON-SPEECH\n",
      "  0:00:00.160000 → 0:00:03.810000 (3.65s): SPEECH\n",
      "    Text: sion tower at departure echo two for turn bravo papa zulu\n",
      "\n",
      "To verify this file:\n",
      "1. Open audio file: VAD_Input\\Audio\\DEV\\atco2_test-set-1h_LSGS_SION_Tower_118_3MHz_20210423_093650-D__000016-000381.wav\n",
      "2. Open ground truth: VAD_Input\\Ground\\atco2_test-set-1h_LSGS_SION_Tower_118_3MHz_20210423_093650-D__000016-000381.txt\n",
      "3. Listen to the audio and check if segments match the timeline above\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "File: atco2_test-set-1h_LZIB_STEFANIK_Tower_118_3MHz_20210429_075711-A__000429-000700.wav\n",
      "--------------------------------------------------------------------------------\n",
      "Total duration: 0:00:02.710000\n",
      "\n",
      "Timeline:\n",
      "  0:00:00 → 0:00:04.290000 (4.29s): NON-SPEECH\n",
      "  0:00:04.290000 → 0:00:07 (2.71s): SPEECH\n",
      "    Text: slovak government zero zero four stefanik tower dobre rano\n",
      "\n",
      "To verify this file:\n",
      "1. Open audio file: VAD_Input\\Audio\\TRAIN\\atco2_test-set-1h_LZIB_STEFANIK_Tower_118_3MHz_20210429_075711-A__000429-000700.wav\n",
      "2. Open ground truth: VAD_Input\\Ground\\atco2_test-set-1h_LZIB_STEFANIK_Tower_118_3MHz_20210429_075711-A__000429-000700.txt\n",
      "3. Listen to the audio and check if segments match the timeline above\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "File: atco2_test-set-1h_LKPR_RUZYNE_Radar_120_520MHz_20201028_092412-G__000575-001095.wav\n",
      "--------------------------------------------------------------------------------\n",
      "Total duration: 0:00:05.200000\n",
      "\n",
      "Timeline:\n",
      "  0:00:00 → 0:00:05.750000 (5.75s): NON-SPEECH\n",
      "  0:00:05.750000 → 0:00:10.950000 (5.20s): SPEECH\n",
      "    Text: turn left to seven zero cleared for ils runway two four oscar uniform romeo\n",
      "\n",
      "To verify this file:\n",
      "1. Open audio file: VAD_Input\\Audio\\TRAIN\\atco2_test-set-1h_LKPR_RUZYNE_Radar_120_520MHz_20201028_092412-G__000575-001095.wav\n",
      "2. Open ground truth: VAD_Input\\Ground\\atco2_test-set-1h_LKPR_RUZYNE_Radar_120_520MHz_20201028_092412-G__000575-001095.txt\n",
      "3. Listen to the audio and check if segments match the timeline above\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "from pydub import AudioSegment\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "\n",
    "def test_segmentation_random_files(n_files=5):\n",
    "    \"\"\"\n",
    "    Test segmentation on n random files and display results for manual verification.\n",
    "    \"\"\"\n",
    "    # Setup paths\n",
    "    base_dir = Path(\"VAD_Input\")\n",
    "    splits = [\"DEV\", \"TEST\", \"TRAIN\"]\n",
    "    \n",
    "    # Collect all audio files\n",
    "    all_audio_files = []\n",
    "    for split in splits:\n",
    "        audio_dir = base_dir / \"Audio\" / split\n",
    "        all_audio_files.extend(list(audio_dir.glob(\"*.wav\")))\n",
    "    \n",
    "    # Randomly select n files\n",
    "    test_files = random.sample(all_audio_files, min(n_files, len(all_audio_files)))\n",
    "    \n",
    "    print(f\"=== Testing {len(test_files)} Random Files ===\\n\")\n",
    "    \n",
    "    for audio_file in test_files:\n",
    "        gt_file = base_dir / \"Ground\" / f\"{audio_file.stem}.txt\"\n",
    "        \n",
    "        print(f\"\\nFile: {audio_file.name}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Load audio\n",
    "        audio = AudioSegment.from_file(str(audio_file))\n",
    "        duration = len(audio) / 1000.0  # Convert to seconds\n",
    "        \n",
    "        # Read ground truth\n",
    "        segments = []\n",
    "        with open(gt_file, 'r') as f:\n",
    "            for line in f:\n",
    "                start, end, text = line.strip().split('\\t')\n",
    "                segments.append({\n",
    "                    'start': float(start),\n",
    "                    'end': float(end),\n",
    "                    'text': text\n",
    "                })\n",
    "        \n",
    "        # Print timeline\n",
    "        print(f\"Total duration: {timedelta(seconds=duration)}\")\n",
    "        print(\"\\nTimeline:\")\n",
    "        current_time = 0.0\n",
    "        \n",
    "        for i, seg in enumerate(segments):\n",
    "            # If there's a gap before this segment, it's non-speech\n",
    "            if current_time < seg['start']:\n",
    "                gap_duration = seg['start'] - current_time\n",
    "                print(f\"  {timedelta(seconds=current_time)} → {timedelta(seconds=seg['start'])} \"\n",
    "                      f\"({gap_duration:.2f}s): NON-SPEECH\")\n",
    "            \n",
    "            # Print speech segment\n",
    "            print(f\"  {timedelta(seconds=seg['start'])} → {timedelta(seconds=seg['end'])} \"\n",
    "                  f\"({seg['end']-seg['start']:.2f}s): SPEECH\")\n",
    "            print(f\"    Text: {seg['text']}\")\n",
    "            \n",
    "            current_time = seg['end']\n",
    "        \n",
    "        # If there's remaining time after last segment\n",
    "        if current_time < duration:\n",
    "            gap_duration = duration - current_time\n",
    "            print(f\"  {timedelta(seconds=current_time)} → {timedelta(seconds=duration)} \"\n",
    "                  f\"({gap_duration:.2f}s): NON-SPEECH\")\n",
    "        \n",
    "        print(\"\\nTo verify this file:\")\n",
    "        print(f\"1. Open audio file: {audio_file}\")\n",
    "        print(f\"2. Open ground truth: {gt_file}\")\n",
    "        print(f\"3. Listen to the audio and check if segments match the timeline above\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    random.seed(42)  # For reproducibility\n",
    "    test_segmentation_random_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Single File ===\n",
      "Audio file: atco2_test-set-1h_LKPR_RUZYNE_Radar_120_520MHz_20201026_145941-G__001153-001308.wav\n",
      "Ground truth: atco2_test-set-1h_LKPR_RUZYNE_Radar_120_520MHz_20201026_145941-G__001153-001308.txt\n",
      "Error processing file: [WinError 2] The system cannot find the file specified\n",
      "Processing failed!\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from pydub import AudioSegment\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "def process_audio_file(audio_file, gt_file, speech_dir, non_speech_dir):\n",
    "    \"\"\"\n",
    "    Process a single audio file and split it into speech/non-speech segments.\n",
    "    Returns the processed segments and their metadata.\n",
    "    \"\"\"\n",
    "    # Load audio file\n",
    "    audio = AudioSegment.from_file(str(audio_file))\n",
    "    total_duration = len(audio) / 1000.0\n",
    "    \n",
    "    # Read and sort ground truth segments\n",
    "    speech_segments = []\n",
    "    with open(gt_file, 'r') as f:\n",
    "        for line in f:\n",
    "            start, end, text = line.strip().split('\\t')\n",
    "            speech_segments.append({\n",
    "                'start': float(start),\n",
    "                'end': float(end),\n",
    "                'text': text\n",
    "            })\n",
    "    speech_segments.sort(key=lambda x: x['start'])\n",
    "    \n",
    "    # Process speech segments\n",
    "    combined_speech = AudioSegment.empty()\n",
    "    speech_metadata = []\n",
    "    for seg in speech_segments:\n",
    "        start_ms = int(seg['start'] * 1000)\n",
    "        end_ms = int(seg['end'] * 1000)\n",
    "        segment = audio[start_ms:end_ms]\n",
    "        combined_speech += segment\n",
    "        speech_metadata.append(seg)\n",
    "    \n",
    "    # Save speech segments\n",
    "    speech_output = speech_dir / f\"{audio_file.stem}_speech.wav\"\n",
    "    combined_speech.export(str(speech_output), format=\"wav\", parameters=[\"-ac\", \"1\"])\n",
    "    \n",
    "    # Process non-speech segments\n",
    "    current_time = 0.0\n",
    "    combined_non_speech = AudioSegment.empty()\n",
    "    non_speech_metadata = []\n",
    "    \n",
    "    for seg in speech_segments:\n",
    "        if current_time < seg['start']:\n",
    "            start_ms = int(current_time * 1000)\n",
    "            end_ms = int(seg['start'] * 1000)\n",
    "            non_speech_segment = audio[start_ms:end_ms]\n",
    "            combined_non_speech += non_speech_segment\n",
    "            non_speech_metadata.append({\n",
    "                'start': current_time,\n",
    "                'end': seg['start']\n",
    "            })\n",
    "        current_time = seg['end']\n",
    "    \n",
    "    # Handle final non-speech segment\n",
    "    if current_time < total_duration:\n",
    "        start_ms = int(current_time * 1000)\n",
    "        end_ms = int(total_duration * 1000)\n",
    "        non_speech_segment = audio[start_ms:end_ms]\n",
    "        combined_non_speech += non_speech_segment\n",
    "        non_speech_metadata.append({\n",
    "            'start': current_time,\n",
    "            'end': total_duration\n",
    "        })\n",
    "    \n",
    "    # Save non-speech segments\n",
    "    non_speech_output = non_speech_dir / f\"{audio_file.stem}_non_speech.wav\"\n",
    "    combined_non_speech.export(str(non_speech_output), format=\"wav\", parameters=[\"-ac\", \"1\"])\n",
    "    \n",
    "    return {\n",
    "        'total_duration': total_duration,\n",
    "        'speech_segments': speech_metadata,\n",
    "        'non_speech_segments': non_speech_metadata,\n",
    "        'speech_file': speech_output,\n",
    "        'non_speech_file': non_speech_output\n",
    "    }\n",
    "\n",
    "def print_metadata(audio_file, metadata):\n",
    "    \"\"\"Print formatted metadata for the processed file.\"\"\"\n",
    "    print(\"\\n=== Metadata ===\")\n",
    "    print(f\"Original file: {audio_file.name}\")\n",
    "    print(f\"Original duration: {timedelta(seconds=metadata['total_duration'])}\")\n",
    "    \n",
    "    print(\"\\nSpeech segments:\")\n",
    "    for seg in metadata['speech_segments']:\n",
    "        print(f\"[{timedelta(seconds=seg['start'])} → {timedelta(seconds=seg['end'])}] \"\n",
    "              f\"Text: {seg['text']}\")\n",
    "    \n",
    "    print(\"\\nNon-speech segments:\")\n",
    "    for seg in metadata['non_speech_segments']:\n",
    "        print(f\"[{timedelta(seconds=seg['start'])} → {timedelta(seconds=seg['end'])}]\")\n",
    "    \n",
    "    print(f\"\\nOutput files:\")\n",
    "    print(f\"Speech file: {metadata['speech_file'].name}\")\n",
    "    print(f\"Non-speech file: {metadata['non_speech_file'].name}\")\n",
    "\n",
    "def test_single_file_split(audio_path, gt_path, output_dir=\"test_output\"):\n",
    "    \"\"\"Test splitting functionality on a single audio/ground truth pair.\"\"\"\n",
    "    try:\n",
    "        # Setup paths\n",
    "        audio_file = Path(audio_path)\n",
    "        gt_file = Path(gt_path)\n",
    "        output_dir = Path(output_dir)\n",
    "        speech_dir = output_dir / \"speech\"\n",
    "        non_speech_dir = output_dir / \"non_speech\"\n",
    "        \n",
    "        # Create output directories\n",
    "        speech_dir.mkdir(parents=True, exist_ok=True)\n",
    "        non_speech_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(\"\\n=== Processing Single File ===\")\n",
    "        print(f\"Audio file: {audio_file.name}\")\n",
    "        print(f\"Ground truth: {gt_file.name}\")\n",
    "        \n",
    "        # Process the audio file\n",
    "        metadata = process_audio_file(audio_file, gt_file, speech_dir, non_speech_dir)\n",
    "        \n",
    "        # Print metadata after processing is complete\n",
    "        print_metadata(audio_file, metadata)\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    audio_file = \"VAD_Input/Audio/DEV/atco2_test-set-1h_LKPR_RUZYNE_Radar_120_520MHz_20201026_145941-G__001153-001308.wav\"\n",
    "    gt_file = \"VAD_Input/Ground/atco2_test-set-1h_LKPR_RUZYNE_Radar_120_520MHz_20201026_145941-G__001153-001308.txt\"\n",
    "    \n",
    "    success = test_single_file_split(audio_file, gt_file)\n",
    "    if not success:\n",
    "        print(\"Processing failed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
